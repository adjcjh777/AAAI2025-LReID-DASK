==========
Args:Namespace(AF_weight=1.0, MODEL='50x', absolute_delta=True, absolute_feat=False, aux_weight=4.5, batch_size=128, blur=False, config_file='config/base.yml', data_dir='/DATA2025/cjh/AAAI2025-LReID-DASK/PRID', dropout=0.5, epochs=60, epochs0=80, eval_epoch=100, evaluate=False, fisher_freeze=True, fisher_ratio=0.5, fisher_sample_num=1000, fix_EMA=0.5, global_alpha=100, groups=1, height=256, joint_test=False, l2sp_weight=0.01, logs_dir='/DATA2025/cjh/AAAI2025-LReID-DASK/output', lr=0.008, middle_test=True, milestones=[30], mobile=True, momentum=0.9, n_kernel=1, num_instances=4, optimizer='SGD', print_freq=200, random_rehearser=False, resume='', save_evaluation=False, seed=0, setting=1, test_folder=None, trans=True, warmup_step=10, weight_decay=0.0001, width=128, workers=8)
==========
+---------------------------+--------+------------+---------+
|            set            | images | identities | cameras |
+---------------------------+--------+------------+---------+
| IncrementalSamples4market |        |            |         |
|           train           | 12936  |    751     |    6    |
|           query           |  3368  |    750     |    6    |
|          gallery          | 15913  |    751     |    6    |
+---------------------------+--------+------------+---------+
+--------------------------------+--------+------------+---------+
|              set               | images | identities | cameras |
+--------------------------------+--------+------------+---------+
| IncrementalSamples4subcuhksysu |        |            |         |
|             train              |  4374  |    942     |    1    |
|             query              |  2900  |    2900    |    1    |
|            gallery             |  5447  |    2900    |    1    |
+--------------------------------+--------+------------+---------+
+-------------------------+--------+------------+---------+
|           set           | images | identities | cameras |
+-------------------------+--------+------------+---------+
| IncrementalSamples4duke |        |            |         |
|          train          | 16522  |    702     |    8    |
|          query          |  2228  |    702     |    8    |
|         gallery         | 17661  |    1110    |    8    |
+-------------------------+--------+------------+---------+
/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
+---------------------------+--------+------------+---------+
|            set            | images | identities | cameras |
+---------------------------+--------+------------+---------+
| IncrementalSamples4msmt17 |        |            |         |
|           train           | 30248  |    1041    |    15   |
|           query           | 11659  |    3060    |    15   |
|          gallery          | 82161  |    3060    |    15   |
+---------------------------+--------+------------+---------+
Checking preprocess conditions...
imgs_labeled_dir exists: True
imgs_detected_dir exists: True
+---------------------------+--------+------------+---------+
|            set            | images | identities | cameras |
+---------------------------+--------+------------+---------+
| IncrementalSamples4cuhk03 |        |            |         |
|           train           |  7368  |    767     |    2    |
|           query           |  1400  |    700     |    2    |
|          gallery          |  5328  |    700     |    2    |
+---------------------------+--------+------------+---------+
/DATA2025/cjh/AAAI2025-LReID-DASK/PRID/SenseReID/test_gallery
+------------------------------+--------+------------+---------+
|             set              | images | identities | cameras |
+------------------------------+--------+------------+---------+
| IncrementalSamples4sensereid |        |            |         |
|            train             |  4428  |    1718    |    2    |
|            query             |  1040  |    521     |    2    |
|           gallery            |  3388  |    1718    |    2    |
+------------------------------+--------+------------+---------+
+-------------------------+--------+------------+---------+
|           set           | images | identities | cameras |
+-------------------------+--------+------------+---------+
| IncrementalSamples4grid |        |            |         |
|          train          |  250   |    125     |    6    |
|          query          |  125   |    125     |    5    |
|         gallery         |  900   |    126     |    5    |
+-------------------------+--------+------------+---------+
+-------------------------+--------+------------+---------+
|           set           | images | identities | cameras |
+-------------------------+--------+------------+---------+
| IncrementalSamples4prid |        |            |         |
|          train          |  200   |    100     |    2    |
|          query          |  100   |    100     |    1    |
|         gallery         |  649   |    649     |    1    |
+-------------------------+--------+------------+---------+
using resnet50 as a backbone
===========building ResNet===========
param fc.weight in pre-trained model does not exist in this model.base
param fc.bias in pre-trained model does not exist in this model.base
using soft triplet loss for training
####### starting training on market1501 #######
Epoch: [0][71/71]	Time 0.530 (0.681)	Loss_ce 6.209 (6.209)	Loss_tp 3.328 (4.110)	
Epoch: [1][71/71]	Time 0.531 (0.614)	Loss_ce 5.933 (6.071)	Loss_tp 1.752 (2.272)	
Epoch: [2][71/71]	Time 0.530 (0.626)	Loss_ce 4.964 (5.453)	Loss_tp 0.955 (1.202)	
Epoch: [3][71/71]	Time 0.531 (0.619)	Loss_ce 3.472 (4.062)	Loss_tp 0.870 (0.850)	
Epoch: [4][71/71]	Time 0.530 (0.615)	Loss_ce 2.009 (2.572)	Loss_tp 0.595 (0.718)	
Epoch: [5][71/71]	Time 0.531 (0.629)	Loss_ce 1.128 (1.529)	Loss_tp 0.551 (0.631)	
Epoch: [6][71/71]	Time 0.534 (0.622)	Loss_ce 0.696 (0.910)	Loss_tp 0.445 (0.520)	
Epoch: [7][71/71]	Time 0.535 (0.621)	Loss_ce 0.438 (0.619)	Loss_tp 0.446 (0.470)	
Epoch: [8][71/71]	Time 0.534 (0.625)	Loss_ce 0.371 (0.445)	Loss_tp 0.356 (0.410)	
Epoch: [9][71/71]	Time 0.535 (0.627)	Loss_ce 0.488 (0.377)	Loss_tp 0.477 (0.390)	
Epoch: [10][71/71]	Time 0.531 (0.627)	Loss_ce 0.345 (0.300)	Loss_tp 0.561 (0.356)	
Epoch: [11][71/71]	Time 0.535 (0.631)	Loss_ce 0.169 (0.261)	Loss_tp 0.196 (0.325)	
Epoch: [12][71/71]	Time 0.531 (0.627)	Loss_ce 0.307 (0.217)	Loss_tp 0.446 (0.296)	
Epoch: [13][71/71]	Time 0.534 (0.620)	Loss_ce 0.169 (0.191)	Loss_tp 0.275 (0.286)	
Epoch: [14][71/71]	Time 0.532 (0.621)	Loss_ce 0.148 (0.172)	Loss_tp 0.229 (0.250)	
Epoch: [15][71/71]	Time 0.533 (0.631)	Loss_ce 0.114 (0.166)	Loss_tp 0.124 (0.239)	
Epoch: [16][71/71]	Time 0.531 (0.632)	Loss_ce 0.167 (0.134)	Loss_tp 0.220 (0.208)	
Epoch: [17][71/71]	Time 0.533 (0.624)	Loss_ce 0.085 (0.128)	Loss_tp 0.254 (0.205)	
Epoch: [18][71/71]	Time 0.532 (0.627)	Loss_ce 0.165 (0.119)	Loss_tp 0.324 (0.190)	
Epoch: [19][71/71]	Time 0.531 (0.630)	Loss_ce 0.161 (0.113)	Loss_tp 0.209 (0.188)	
Epoch: [20][71/71]	Time 0.536 (0.630)	Loss_ce 0.049 (0.103)	Loss_tp 0.127 (0.174)	
Epoch: [21][71/71]	Time 0.532 (0.623)	Loss_ce 0.101 (0.093)	Loss_tp 0.201 (0.159)	
Epoch: [22][71/71]	Time 0.531 (0.627)	Loss_ce 0.103 (0.090)	Loss_tp 0.253 (0.155)	
Epoch: [23][71/71]	Time 0.534 (0.630)	Loss_ce 0.127 (0.076)	Loss_tp 0.198 (0.125)	
Epoch: [24][71/71]	Time 0.533 (0.629)	Loss_ce 0.108 (0.081)	Loss_tp 0.157 (0.130)	
Epoch: [25][71/71]	Time 0.532 (0.631)	Loss_ce 0.139 (0.075)	Loss_tp 0.141 (0.129)	
Epoch: [26][71/71]	Time 0.534 (0.626)	Loss_ce 0.045 (0.074)	Loss_tp 0.074 (0.113)	
Epoch: [27][71/71]	Time 0.531 (0.621)	Loss_ce 0.077 (0.069)	Loss_tp 0.135 (0.112)	
Epoch: [28][71/71]	Time 0.530 (0.623)	Loss_ce 0.099 (0.064)	Loss_tp 0.122 (0.110)	
Epoch: [29][71/71]	Time 0.532 (0.628)	Loss_ce 0.039 (0.060)	Loss_tp 0.058 (0.102)	
Epoch: [30][71/71]	Time 0.531 (0.621)	Loss_ce 0.038 (0.053)	Loss_tp 0.064 (0.094)	
Epoch: [31][71/71]	Time 0.532 (0.621)	Loss_ce 0.053 (0.046)	Loss_tp 0.099 (0.083)	
Epoch: [32][71/71]	Time 0.533 (0.627)	Loss_ce 0.017 (0.042)	Loss_tp 0.022 (0.074)	
Epoch: [33][71/71]	Time 0.532 (0.619)	Loss_ce 0.040 (0.040)	Loss_tp 0.059 (0.069)	
Epoch: [34][71/71]	Time 0.537 (0.625)	Loss_ce 0.021 (0.043)	Loss_tp 0.080 (0.075)	
Epoch: [35][71/71]	Time 0.532 (0.634)	Loss_ce 0.039 (0.042)	Loss_tp 0.077 (0.072)	
Epoch: [36][71/71]	Time 0.532 (0.623)	Loss_ce 0.053 (0.039)	Loss_tp 0.037 (0.059)	
Epoch: [37][71/71]	Time 0.532 (0.619)	Loss_ce 0.028 (0.036)	Loss_tp 0.037 (0.063)	
Epoch: [38][71/71]	Time 0.531 (0.627)	Loss_ce 0.041 (0.036)	Loss_tp 0.093 (0.055)	
Epoch: [39][71/71]	Time 0.532 (0.620)	Loss_ce 0.049 (0.035)	Loss_tp 0.042 (0.056)	
Epoch: [40][71/71]	Time 0.531 (0.624)	Loss_ce 0.041 (0.035)	Loss_tp 0.054 (0.057)	
Epoch: [41][71/71]	Time 0.532 (0.628)	Loss_ce 0.041 (0.035)	Loss_tp 0.073 (0.055)	
Epoch: [42][71/71]	Time 0.535 (0.624)	Loss_ce 0.071 (0.036)	Loss_tp 0.093 (0.058)	
Epoch: [43][71/71]	Time 0.533 (0.637)	Loss_ce 0.034 (0.032)	Loss_tp 0.019 (0.051)	
Epoch: [44][71/71]	Time 0.530 (0.631)	Loss_ce 0.014 (0.034)	Loss_tp 0.012 (0.052)	
Epoch: [45][71/71]	Time 0.534 (0.629)	Loss_ce 0.037 (0.031)	Loss_tp 0.123 (0.057)	
Epoch: [46][71/71]	Time 0.532 (0.623)	Loss_ce 0.049 (0.035)	Loss_tp 0.049 (0.052)	
Epoch: [47][71/71]	Time 0.531 (0.634)	Loss_ce 0.056 (0.030)	Loss_tp 0.050 (0.048)	
Epoch: [48][71/71]	Time 0.531 (0.614)	Loss_ce 0.032 (0.031)	Loss_tp 0.053 (0.052)	
Epoch: [49][71/71]	Time 0.533 (0.635)	Loss_ce 0.017 (0.032)	Loss_tp 0.020 (0.045)	
Epoch: [50][71/71]	Time 0.532 (0.624)	Loss_ce 0.033 (0.033)	Loss_tp 0.018 (0.049)	
Epoch: [51][71/71]	Time 0.530 (0.625)	Loss_ce 0.009 (0.029)	Loss_tp 0.014 (0.053)	
Epoch: [52][71/71]	Time 0.533 (0.637)	Loss_ce 0.033 (0.032)	Loss_tp 0.037 (0.054)	
Epoch: [53][71/71]	Time 0.532 (0.634)	Loss_ce 0.057 (0.031)	Loss_tp 0.030 (0.049)	
Epoch: [54][71/71]	Time 0.533 (0.624)	Loss_ce 0.026 (0.028)	Loss_tp 0.023 (0.044)	
Epoch: [55][71/71]	Time 0.531 (0.622)	Loss_ce 0.042 (0.027)	Loss_tp 0.093 (0.043)	
Epoch: [56][71/71]	Time 0.531 (0.633)	Loss_ce 0.046 (0.027)	Loss_tp 0.042 (0.044)	
Epoch: [57][71/71]	Time 0.531 (0.622)	Loss_ce 0.016 (0.028)	Loss_tp 0.031 (0.044)	
Epoch: [58][71/71]	Time 0.533 (0.620)	Loss_ce 0.031 (0.030)	Loss_tp 0.091 (0.047)	
Epoch: [59][71/71]	Time 0.531 (0.627)	Loss_ce 0.027 (0.029)	Loss_tp 0.068 (0.051)	
Epoch: [60][71/71]	Time 0.529 (0.626)	Loss_ce 0.065 (0.030)	Loss_tp 0.062 (0.044)	
Epoch: [61][71/71]	Time 0.534 (0.625)	Loss_ce 0.045 (0.028)	Loss_tp 0.069 (0.043)	
Epoch: [62][71/71]	Time 0.534 (0.631)	Loss_ce 0.015 (0.027)	Loss_tp 0.012 (0.041)	
Epoch: [63][71/71]	Time 0.534 (0.624)	Loss_ce 0.024 (0.031)	Loss_tp 0.033 (0.043)	
Epoch: [64][71/71]	Time 0.534 (0.623)	Loss_ce 0.027 (0.027)	Loss_tp 0.047 (0.044)	
Epoch: [65][71/71]	Time 0.531 (0.624)	Loss_ce 0.032 (0.027)	Loss_tp 0.043 (0.039)	
Epoch: [66][71/71]	Time 0.534 (0.622)	Loss_ce 0.025 (0.026)	Loss_tp 0.048 (0.041)	
Epoch: [67][71/71]	Time 0.532 (0.625)	Loss_ce 0.052 (0.026)	Loss_tp 0.076 (0.042)	
Epoch: [68][71/71]	Time 0.530 (0.624)	Loss_ce 0.017 (0.028)	Loss_tp 0.019 (0.047)	
Epoch: [69][71/71]	Time 0.530 (0.623)	Loss_ce 0.015 (0.025)	Loss_tp 0.018 (0.044)	
Epoch: [70][71/71]	Time 0.537 (0.629)	Loss_ce 0.009 (0.025)	Loss_tp 0.016 (0.035)	
Epoch: [71][71/71]	Time 0.532 (0.644)	Loss_ce 0.018 (0.023)	Loss_tp 0.047 (0.034)	
Epoch: [72][71/71]	Time 0.534 (0.630)	Loss_ce 0.041 (0.028)	Loss_tp 0.080 (0.046)	
Epoch: [73][71/71]	Time 0.532 (0.631)	Loss_ce 0.055 (0.028)	Loss_tp 0.096 (0.042)	
Epoch: [74][71/71]	Time 0.529 (0.630)	Loss_ce 0.016 (0.029)	Loss_tp 0.049 (0.043)	
Epoch: [75][71/71]	Time 0.533 (0.626)	Loss_ce 0.030 (0.021)	Loss_tp 0.048 (0.037)	
Epoch: [76][71/71]	Time 0.531 (0.625)	Loss_ce 0.022 (0.024)	Loss_tp 0.024 (0.035)	
Epoch: [77][71/71]	Time 0.531 (0.635)	Loss_ce 0.016 (0.025)	Loss_tp 0.020 (0.036)	
Epoch: [78][71/71]	Time 0.538 (0.620)	Loss_ce 0.018 (0.024)	Loss_tp 0.007 (0.039)	
Epoch: [79][71/71]	Time 0.533 (0.632)	Loss_ce 0.028 (0.023)	Loss_tp 0.039 (0.039)	
=> saved checkpoint '/DATA2025/cjh/AAAI2025-LReID-DASK/output/market1501_checkpoint.pth.tar'
****** start perform fast testing! ******
2025-12-15 11:02:59  market1501 feature start
2025-12-15 11:03:34  market1501 feature done
fast testing!!!
mAP/Rank1:	78.5/90.4
2025-12-15 11:03:41  sense feature start
2025-12-15 11:03:49  sense feature done
fast testing!!!
mAP/Rank1:	35.4/28.0
2025-12-15 11:03:49  grid feature start
2025-12-15 11:03:53  grid feature done
fast testing!!!
mAP/Rank1:	21.6/15.2
2025-12-15 11:03:53  prid feature start
2025-12-15 11:03:55  prid feature done
fast testing!!!
mAP/Rank1:	30.4/23.0
Average mAP on Seen dataset: 78.5
Average R1 on Seen dataset: 90.4
market1501		|Average	|
|78.5/90.4	|78.5/90.4	|
Average mAP on UnSeen dataset: 29.1
Average R1 on UnSeen dataset: 22.1
sense	grid	prid	|Average	|
|35.4/28.0	|21.6/15.2	|30.4/23.0	|29.1/22.1	|
78.5	90.4	29.1	22.1
=> saved checkpoint '/DATA2025/cjh/AAAI2025-LReID-DASK/output/market1501_checkpoint.pth.tar'
Computing Fisher Information Matrix...
/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Fisher Matrix Computed on 1024 samples.
Param count for AKPNet's initialized parameters: 1105348
=> Loaded checkpoint 'rehearser_pretrain_learn_kernel_c1-g1_mobilenet-v3/market1501_rehearser_49.pth.tar'
Applying Fisher-based Freezing (EWC-style)...
Fisher Threshold: 4.388071295213791e-10 (Top 50.0%)
Gradient mask prepared. Approx 12268064 parameters frozen.
using soft triplet loss for training
####### starting training on cuhk_sysu #######
Epoch: [0][17/17]	Time 1.579 (1.777)	Loss_ce 4.103 (3.869)	Loss_tp 0.764 (0.885)	
Epoch: [1][17/17]	Time 1.618 (1.778)	Loss_ce 1.783 (2.715)	Loss_tp 0.419 (0.674)	
Epoch: [2][17/17]	Time 1.588 (1.769)	Loss_ce 0.761 (1.202)	Loss_tp 0.301 (0.475)	
Epoch: [3][17/17]	Time 1.634 (1.766)	Loss_ce 0.205 (0.470)	Loss_tp 0.164 (0.312)	
Epoch: [4][17/17]	Time 1.677 (1.782)	Loss_ce 0.183 (0.214)	Loss_tp 0.133 (0.253)	
Epoch: [5][17/17]	Time 1.575 (1.765)	Loss_ce 0.043 (0.122)	Loss_tp 0.079 (0.175)	
Epoch: [6][17/17]	Time 1.589 (1.760)	Loss_ce 0.040 (0.097)	Loss_tp 0.052 (0.157)	
Epoch: [7][17/17]	Time 1.606 (1.795)	Loss_ce 0.029 (0.074)	Loss_tp 0.030 (0.105)	
Epoch: [8][17/17]	Time 1.596 (1.747)	Loss_ce 0.020 (0.066)	Loss_tp 0.027 (0.118)	
Epoch: [9][17/17]	Time 1.609 (1.787)	Loss_ce 0.023 (0.075)	Loss_tp 0.036 (0.104)	
Epoch: [10][17/17]	Time 1.606 (1.796)	Loss_ce 0.014 (0.070)	Loss_tp 0.013 (0.086)	
Epoch: [11][17/17]	Time 1.593 (1.773)	Loss_ce 0.009 (0.054)	Loss_tp 0.035 (0.092)	
Epoch: [12][17/17]	Time 1.611 (1.766)	Loss_ce 0.018 (0.070)	Loss_tp 0.013 (0.083)	
Epoch: [13][17/17]	Time 1.606 (1.748)	Loss_ce 0.012 (0.073)	Loss_tp 0.024 (0.085)	
Epoch: [14][17/17]	Time 1.669 (1.831)	Loss_ce 0.008 (0.050)	Loss_tp 0.029 (0.070)	
Epoch: [15][17/17]	Time 1.598 (1.793)	Loss_ce 0.009 (0.040)	Loss_tp 0.027 (0.079)	
Epoch: [16][17/17]	Time 1.607 (1.749)	Loss_ce 0.007 (0.035)	Loss_tp 0.019 (0.067)	
Epoch: [17][17/17]	Time 1.597 (1.790)	Loss_ce 0.006 (0.036)	Loss_tp 0.026 (0.086)	
Epoch: [18][17/17]	Time 1.589 (1.790)	Loss_ce 0.011 (0.035)	Loss_tp 0.034 (0.060)	
Epoch: [19][17/17]	Time 1.630 (1.787)	Loss_ce 0.005 (0.032)	Loss_tp 0.014 (0.054)	
Epoch: [20][17/17]	Time 1.581 (1.788)	Loss_ce 0.004 (0.049)	Loss_tp 0.008 (0.057)	
Epoch: [21][17/17]	Time 1.599 (1.772)	Loss_ce 0.004 (0.030)	Loss_tp 0.010 (0.049)	
Epoch: [22][17/17]	Time 1.590 (1.773)	Loss_ce 0.015 (0.026)	Loss_tp 0.013 (0.057)	
Epoch: [23][17/17]	Time 1.590 (1.779)	Loss_ce 0.004 (0.023)	Loss_tp 0.013 (0.046)	
Epoch: [24][17/17]	Time 1.566 (1.763)	Loss_ce 0.092 (0.045)	Loss_tp 0.025 (0.068)	
Epoch: [25][17/17]	Time 1.676 (1.760)	Loss_ce 0.005 (0.060)	Loss_tp 0.019 (0.048)	
Epoch: [26][17/17]	Time 1.679 (1.769)	Loss_ce 0.005 (0.020)	Loss_tp 0.037 (0.055)	
Epoch: [27][17/17]	Time 1.605 (1.794)	Loss_ce 0.004 (0.030)	Loss_tp 0.012 (0.051)	
Epoch: [28][17/17]	Time 1.602 (1.776)	Loss_ce 0.004 (0.030)	Loss_tp 0.021 (0.070)	
Epoch: [29][17/17]	Time 1.622 (1.767)	Loss_ce 0.003 (0.019)	Loss_tp 0.017 (0.054)	
Epoch: [30][17/17]	Time 1.597 (1.753)	Loss_ce 0.004 (0.032)	Loss_tp 0.017 (0.048)	
Epoch: [31][17/17]	Time 1.640 (1.790)	Loss_ce 0.006 (0.021)	Loss_tp 0.011 (0.041)	
Epoch: [32][17/17]	Time 1.588 (1.770)	Loss_ce 0.005 (0.024)	Loss_tp 0.013 (0.054)	
Epoch: [33][17/17]	Time 1.606 (1.777)	Loss_ce 0.004 (0.023)	Loss_tp 0.011 (0.051)	
Epoch: [34][17/17]	Time 1.618 (1.800)	Loss_ce 0.003 (0.018)	Loss_tp 0.008 (0.051)	
Epoch: [35][17/17]	Time 1.598 (1.774)	Loss_ce 0.002 (0.019)	Loss_tp 0.004 (0.044)	
Epoch: [36][17/17]	Time 1.589 (1.755)	Loss_ce 0.003 (0.016)	Loss_tp 0.007 (0.042)	
Epoch: [37][17/17]	Time 1.606 (1.780)	Loss_ce 0.003 (0.017)	Loss_tp 0.021 (0.048)	
Epoch: [38][17/17]	Time 1.565 (1.788)	Loss_ce 0.004 (0.016)	Loss_tp 0.014 (0.049)	
Epoch: [39][17/17]	Time 1.587 (1.783)	Loss_ce 0.002 (0.018)	Loss_tp 0.018 (0.051)	
Epoch: [40][17/17]	Time 1.599 (1.758)	Loss_ce 0.003 (0.019)	Loss_tp 0.010 (0.064)	
Epoch: [41][17/17]	Time 1.606 (1.792)	Loss_ce 0.002 (0.014)	Loss_tp 0.006 (0.041)	
Epoch: [42][17/17]	Time 1.598 (1.764)	Loss_ce 0.004 (0.015)	Loss_tp 0.017 (0.038)	
Epoch: [43][17/17]	Time 1.605 (1.802)	Loss_ce 0.002 (0.016)	Loss_tp 0.005 (0.050)	
Epoch: [44][17/17]	Time 1.622 (1.787)	Loss_ce 0.002 (0.017)	Loss_tp 0.012 (0.044)	
Epoch: [45][17/17]	Time 1.637 (1.760)	Loss_ce 0.003 (0.015)	Loss_tp 0.018 (0.046)	
Epoch: [46][17/17]	Time 1.656 (1.792)	Loss_ce 0.003 (0.016)	Loss_tp 0.009 (0.047)	
Epoch: [47][17/17]	Time 1.599 (1.826)	Loss_ce 0.004 (0.018)	Loss_tp 0.040 (0.067)	
Epoch: [48][17/17]	Time 1.606 (1.765)	Loss_ce 0.002 (0.015)	Loss_tp 0.011 (0.053)	
Epoch: [49][17/17]	Time 1.604 (1.799)	Loss_ce 0.003 (0.014)	Loss_tp 0.012 (0.059)	
Epoch: [50][17/17]	Time 1.691 (1.779)	Loss_ce 0.003 (0.014)	Loss_tp 0.015 (0.051)	
Epoch: [51][17/17]	Time 1.603 (1.762)	Loss_ce 0.004 (0.016)	Loss_tp 0.013 (0.040)	
Epoch: [52][17/17]	Time 1.608 (1.754)	Loss_ce 0.036 (0.021)	Loss_tp 0.027 (0.047)	
Epoch: [53][17/17]	Time 1.646 (1.807)	Loss_ce 0.002 (0.017)	Loss_tp 0.011 (0.060)	
Epoch: [54][17/17]	Time 1.603 (1.779)	Loss_ce 0.002 (0.013)	Loss_tp 0.010 (0.040)	
Epoch: [55][17/17]	Time 1.584 (1.739)	Loss_ce 0.005 (0.016)	Loss_tp 0.009 (0.048)	
Epoch: [56][17/17]	Time 1.626 (1.753)	Loss_ce 0.002 (0.016)	Loss_tp 0.011 (0.044)	
Epoch: [57][17/17]	Time 1.607 (1.779)	Loss_ce 0.003 (0.014)	Loss_tp 0.018 (0.040)	
Epoch: [58][17/17]	Time 1.633 (1.747)	Loss_ce 0.003 (0.016)	Loss_tp 0.011 (0.047)	
Epoch: [59][17/17]	Time 1.594 (1.784)	Loss_ce 0.004 (0.013)	Loss_tp 0.010 (0.045)	
=> saved checkpoint '/DATA2025/cjh/AAAI2025-LReID-DASK/output/cuhk_sysu_checkpoint.pth.tar'
****** start perform fast testing! ******
2025-12-15 11:35:08  market1501 feature start
2025-12-15 11:35:41  market1501 feature done
fast testing!!!
mAP/Rank1:	73.4/88.7
2025-12-15 11:35:47  cuhk_sysu feature start
2025-12-15 11:36:03  cuhk_sysu feature done
fast testing!!!
mAP/Rank1:	81.4/83.2
2025-12-15 11:36:05  sense feature start
2025-12-15 11:36:14  sense feature done
fast testing!!!
mAP/Rank1:	39.6/31.1
2025-12-15 11:36:14  grid feature start
2025-12-15 11:36:17  grid feature done
fast testing!!!
mAP/Rank1:	33.2/22.4
2025-12-15 11:36:17  prid feature start
2025-12-15 11:36:20  prid feature done
fast testing!!!
mAP/Rank1:	40.6/35.0
Average mAP on Seen dataset: 77.4
Average R1 on Seen dataset: 86.0
market1501		cuhk_sysu		|Average	|
|73.4/88.7	|81.4/83.2	|77.4/86.0	|
Average mAP on UnSeen dataset: 37.8
Average R1 on UnSeen dataset: 29.5
sense	grid	prid	|Average	|
|39.6/31.1	|33.2/22.4	|40.6/35.0	|37.8/29.5	|
77.4	86.0	37.8	29.5
=> saved checkpoint '/DATA2025/cjh/AAAI2025-LReID-DASK/output/cuhk_sysu_checkpoint.pth.tar'
Computing Fisher Information Matrix...
Fisher Matrix Computed on 1024 samples.
*******combining the models with alpha: 0.5*******
module.classifier.weight ...
****** start perform fast testing! ******
2025-12-15 11:36:35  market1501 feature start
2025-12-15 11:37:07  market1501 feature done
fast testing!!!
mAP/Rank1:	78.2/90.9
2025-12-15 11:37:13  cuhk_sysu feature start
2025-12-15 11:37:28  cuhk_sysu feature done
fast testing!!!
mAP/Rank1:	78.5/80.8
2025-12-15 11:37:30  sense feature start
2025-12-15 11:37:38  sense feature done
fast testing!!!
mAP/Rank1:	39.4/31.9
2025-12-15 11:37:39  grid feature start
2025-12-15 11:37:42  grid feature done
fast testing!!!
mAP/Rank1:	27.3/19.2
2025-12-15 11:37:42  prid feature start
2025-12-15 11:37:45  prid feature done
fast testing!!!
mAP/Rank1:	35.1/27.0
Average mAP on Seen dataset: 78.3
Average R1 on Seen dataset: 85.9
market1501		cuhk_sysu		|Average	|
|78.2/90.9	|78.5/80.8	|78.3/85.9	|
Average mAP on UnSeen dataset: 33.9
Average R1 on UnSeen dataset: 26.0
sense	grid	prid	|Average	|
|39.4/31.9	|27.3/19.2	|35.1/27.0	|33.9/26.0	|
78.3	85.9	33.9	26.0
Param count for AKPNet's initialized parameters: 1105348
=> Loaded checkpoint 'rehearser_pretrain_learn_kernel_c1-g1_mobilenet-v3/cuhk_sysu_rehearser_49.pth.tar'
Applying Fisher-based Freezing (EWC-style)...
Fisher Threshold: 0.00026164489099755883 (Top 50.0%)
Gradient mask prepared. Approx 12780065 parameters frozen.
using soft triplet loss for training
####### starting training on dukemtmc #######
Traceback (most recent call last):
  File "continual_train.py", line 616, in <module>
    parser.add_argument('--fisher-freeze', action='store_true', help="Enable Fisher-based parameter freezing")
  File "continual_train.py", line 53, in main
    main_worker(args, cfg)
  File "continual_train.py", line 176, in main_worker
  File "continual_train.py", line 484, in train_dataset
    rehearser=rehearser_list[-1]
  File "/DATA2025/cjh/AAAI2025-LReID-DASK/reid/trainer.py", line 120, in train
    s_features_old, bn_feat_old, cls_outputs_old, feat_final_layer_old = old_model(s_inputs, get_all_feat=True)
  File "/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 171, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 181, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 89, in parallel_apply
    output.reraise()
  File "/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torch/_utils.py", line 543, in reraise
    raise exception
torch.cuda.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 64, in _worker
    output = module(*input, **kwargs)
  File "/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/DATA2025/cjh/AAAI2025-LReID-DASK/reid/models/resnet.py", line 89, in forward
    x = self.base(x)
  File "/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/DATA2025/cjh/AAAI2025-LReID-DASK/reid/models/backbones/resnet.py", line 223, in forward
    x = self.layer1(x)
  File "/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/DATA2025/cjh/AAAI2025-LReID-DASK/reid/models/backbones/resnet.py", line 178, in forward
    residual = self.downsample(x)
  File "/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/DATA2025/cjh/envs/IRL/lib/python3.8/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 9.92 GiB already allocated; 76.50 MiB free; 10.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF